{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab47790-1a8a-4e91-aba7-e5a307b89c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5fed7d3-735a-4c16-bc1b-9cc6b5addf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b1c7a7-799c-4f84-af3f-36b090a6e80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "print(torch.__version__)\n",
    "print(hasattr(models, \"resnet50\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ad010-9111-4be6-8781-b2cba93e6838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: 10015\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(\n",
    "    os.path.join(os.path.dirname(__file__), \"..\")\n",
    ")\n",
    "\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"raw\", \"HAM10000\")\n",
    "IMAGE_DIR = os.path.join(DATA_DIR, \"images\")\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"metadata.csv\")\n",
    "\n",
    "assert os.path.exists(DATA_DIR), \"DATA_DIR does not exist\"\n",
    "assert os.path.exists(IMAGE_DIR), \"IMAGE_DIR does not exist\"\n",
    "assert os.path.exists(CSV_PATH), \"CSV_PATH does not exist\"\n",
    "\n",
    "print(\"Images:\", len(os.listdir(IMAGE_DIR)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e2470e3-f345-4dc6-abbc-0fd89b994b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISIC_0027419\n",
      "/Users/harrixide/Desktop/hackathon_project/Skin_lession_cnn_analysis/data/raw/HAM10000/images/ISIC_0027419.jpg\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "example_id = df.iloc[0][\"image_id\"]\n",
    "example_path = os.path.join(IMAGE_DIR, example_id + \".jpg\")\n",
    "\n",
    "print(example_id)\n",
    "print(example_path)\n",
    "print(os.path.exists(example_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fad790dc-3a6c-46c5-980f-661cb8d45fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENIGN = {\"nv\", \"bkl\", \"df\", \"vasc\"}\n",
    "MALIGNANT = {\"mel\", \"bcc\", \"akiec\"}\n",
    "\n",
    "LABEL_MAP = {k: 0 for k in BENIGN}\n",
    "LABEL_MAP.update({k: 1 for k in MALIGNANT})\n",
    "\n",
    "class HAMDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_dir, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.df = self.df[self.df[\"dx\"].isin(LABEL_MAP)]\n",
    "        self.df[\"label\"] = self.df[\"dx\"].map(LABEL_MAP)\n",
    "\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx): #here is exactly where we combine labels and images\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row[\"image_id\"] + \".jpg\")\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = row[\"label\"]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0954e40a-1004-426e-a762-cbef38818c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4b575d0-36c8-4f23-b96b-1edd67d3684f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.float32\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dataset = HAMDataset(CSV_PATH, IMAGE_DIR, train_transform)\n",
    "\n",
    "img, label = dataset[0]\n",
    "print(img.shape)\n",
    "print(img.dtype)\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "194b25a4-a397-47c7-a77d-ce164462ce05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8012 2003\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset,\n",
    "    [train_size, val_size]\n",
    ")\n",
    "\n",
    "print(len(train_dataset), len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bffc8886-cc89-47b4-a1d4-e4207a5703ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f23c6c0d-90a1-4d47-96ff-246e04d42d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42364001-33ae-4eb5-975c-9ef6868827f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrixide/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/harrixide/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe3f084-63a7-49dd-9d96-9486f7fae74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "684ad970-e34f-460f-b588-dac1c8bf6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.fc.parameters(),\n",
    "    lr=1e-3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5613b54c-1490-4b82-b923-350b7817eed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "outputs = model(images)\n",
    "\n",
    "print(outputs.shape)   # should be [batch_size, 2]\n",
    "print(labels.shape)    # should be [batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adf6885c-e0d4-4a41-a4fe-08b1614161e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HAMDataset working\n",
    "#train_dataset, val_dataset\n",
    "#train_loader, val_loader\n",
    "#model on CPU\n",
    "#criterion\n",
    "#optimizer\n",
    "#forward pass verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f33f908-4e14-4c5c-b9ab-bcaaae9b955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4faf49d6-1952-4de7-a924-06aae06f3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7163ab3c-106e-42be-b898-b782cab6ed2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Train Loss: 0.3878, Train Acc: 0.8280 | Val Loss: 0.3144, Val Acc: 0.8527\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc = evaluate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ddc5d1f-268f-45a5-b360-e5a00e6e4ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_image(model, image_path, transform, device):\n",
    "    model.eval()\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "    confidence, pred = torch.max(probs, 1)\n",
    "\n",
    "    label = \"Malignant\" if pred.item() == 1 else \"Benign\"\n",
    "    return label, confidence.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cae9ef97-427a-4e8e-87ae-7fd47891c630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign 56.48%\n"
     ]
    }
   ],
   "source": [
    "test_image = os.path.join(IMAGE_DIR, df.iloc[10][\"image_id\"] + \".jpg\")\n",
    "\n",
    "label, confidence = predict_image(\n",
    "    model,\n",
    "    test_image,\n",
    "    train_transform,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(label, f\"{confidence*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c860d1d0-e3f2-4804-a054-5662b1584f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image ID: ISIC_0025276\n",
      "dx: bkl\n",
      "Ground truth label: 0\n",
      "Ground truth class: Benign\n"
     ]
    }
   ],
   "source": [
    "row = df.iloc[10]\n",
    "\n",
    "print(\"Image ID:\", row[\"image_id\"])\n",
    "print(\"dx:\", row[\"dx\"])\n",
    "\n",
    "LABEL_MAP = {\n",
    "    \"nv\": 0,\n",
    "    \"bkl\": 0,\n",
    "    \"df\": 0,\n",
    "    \"vasc\": 0,\n",
    "    \"mel\": 1,\n",
    "    \"bcc\": 1,\n",
    "    \"akiec\": 1\n",
    "}\n",
    "\n",
    "true_label = LABEL_MAP[row[\"dx\"]]\n",
    "\n",
    "print(\"Ground truth label:\", true_label)\n",
    "print(\"Ground truth class:\", \"Benign\" if true_label == 0 else \"Malignant\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
